<!DOCTYPE html><html lang="zh-cn,en,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 2 · NeverSame.Stream</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 2 - robot9"><meta name="keywords"><meta name="author" content="robot9"><link rel="short icon" href="/static/favicon.ico"><link rel="stylesheet" href="/static/site.css"><link rel="search" type="application/opensearchdescription+xml" href="http://neversame.stream/atom.xml" title="NeverSame.Stream"><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="NeverSame.Stream" type="application/atom+xml">
</head><body><header><div class="header row"> <a href="/" class="logo-link"><img src="/static/logo.png"></a><ul id="nav_list" class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div><div id="nav_btn" class="nav-btn"><span></span><span></span><span></span></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 2</h1><div class="post-info">2021-03-18<p class="visit"><i data-identity="2021/03/18/Trending-Apps-从头开始分析流行应用-采集篇-环境设置-2/" class="article-timer"></i><span>次访问</span></p></div><div class="post-content"><p>Link to previous chapter: <a href="/2021/03/17/Trending-Apps-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E5%88%86%E6%9E%90%E6%B5%81%E8%A1%8C%E5%BA%94%E7%94%A8-%E9%87%87%E9%9B%86%E7%AF%87-%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE-1/">环境设置-1</a></p>
<h3 id="2-开始第一个-Scarpy-项目"><a href="#2-开始第一个-Scarpy-项目" class="headerlink" title="2. 开始第一个 Scarpy 项目"></a>2. 开始第一个 Scarpy 项目</h3><h4 id="2-1-创建项目"><a href="#2-1-创建项目" class="headerlink" title="2.1 创建项目"></a>2.1 创建项目</h4><p>因为我们本地并没有scrapy的executable，需要启动一个ad-hoc的容器并在容器里面操作， 进入容器：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; docker-compose run --rm scrapy</span><br></pre></td></tr></table></figure>

<p>现在开始创建我们的project</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; cd &#x2F;app</span><br><span class="line">&gt; scrapy startproject app_trend</span><br><span class="line">&gt; cd app_trend</span><br></pre></td></tr></table></figure>
<p>这样子会生成一个 <code>app_trend</code> folder, 其中主要的文件目录是:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tutorial&#x2F;</span><br><span class="line">    scrapy.cfg            # deploy configuration file</span><br><span class="line">    tutorial&#x2F;             # project&#39;s Python module, you&#39;ll import your code from here</span><br><span class="line">        __init__.py</span><br><span class="line">        items.py          # project items definition file</span><br><span class="line">        middlewares.py    # project middlewares file</span><br><span class="line">        pipelines.py      # project pipelines file</span><br><span class="line">        settings.py       # project settings file</span><br><span class="line">        spiders&#x2F;          # !!! a directory where you&#39;ll later put your spiders</span><br><span class="line">            __init__.py</span><br><span class="line">            </span><br></pre></td></tr></table></figure>
<p>我们将会在 <code>spiders/</code> folder 里面编写我们自己的 spider</p>
<span id="more"></span>

<h4 id="2-2-第一个-spider"><a href="#2-2-第一个-spider" class="headerlink" title="2.2 第一个 spider"></a>2.2 第一个 spider</h4><p>Filename: <code>spiders/top50_spider.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class Top50Spider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;top50&quot;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        urls &#x3D; [</span><br><span class="line">            &#39;https:&#x2F;&#x2F;www.appannie.com&#x2F;en&#x2F;apps&#x2F;ios&#x2F;top&#x2F;&#39;</span><br><span class="line">        ]</span><br><span class="line">        for url in urls:</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                url&#x3D;url, callback&#x3D;self.parse,</span><br><span class="line">                headers&#x3D;&#123;</span><br><span class="line">                    &quot;accept&quot;: &quot;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9&quot;,</span><br><span class="line">                    &quot;accept-language&quot;: &quot;zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8,en-GB;q&#x3D;0.7,en-US;q&#x3D;0.6,zh-TW;q&#x3D;0.5,ja;q&#x3D;0.4&quot;,</span><br><span class="line">                    &quot;accept-encoding&quot;: &quot;gzip, deflate, br&quot;,</span><br><span class="line">                    &quot;cache-control&quot;: &quot;max-age&#x3D;0&quot;,</span><br><span class="line">                    &quot;sec-fetch-dest&quot;: &quot;document&quot;,</span><br><span class="line">                    &quot;sec-fetch-mode&quot;: &quot;navigate&quot;,</span><br><span class="line">                    &quot;sec-fetch-site&quot;: &quot;none&quot;,</span><br><span class="line">                    &quot;sec-fetch-user&quot;: &quot;?1&quot;,</span><br><span class="line">                    &quot;upgrade-insecure-requests&quot;: &quot;1&quot;,</span><br><span class="line">                    &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;89.0.4389.82 Safari&#x2F;537.36 Edg&#x2F;89.0.774.50&quot;,</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for ranking in response.xpath(&#39;&#x2F;&#x2F;div[count(a[contains(@href, &quot;en&#x2F;apps&quot;)])&gt;&#x3D;40]&#39;):</span><br><span class="line">            category &#x3D; ranking.css(&quot;h4::text&quot;).get()</span><br><span class="line">            for app in ranking.xpath(&#39;.&#x2F;&#x2F;a[contains(@href, &quot;en&#x2F;apps&quot;)]&#39;):</span><br><span class="line">                items &#x3D; app.xpath(&#39;.&#x2F;&#x2F;p&#x2F;text()&#39;).getall()</span><br><span class="line">                yield &#123;</span><br><span class="line">                    &#39;category&#39;: category,</span><br><span class="line">                    &#39;rank&#39;: items[0],</span><br><span class="line">                    &#39;name&#39;: items[1],</span><br><span class="line">                    &#39;company&#39;: items[2],</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure>

<div class="tip">
    这里面特别加上了一些headers，特别是`user-agent`是为了防止服务器认为我们是机器人而直接返回503 Error
</div>

<h4 id="2-2-1-网页分析：xpath-和-css-selector"><a href="#2-2-1-网页分析：xpath-和-css-selector" class="headerlink" title="2.2.1 网页分析：xpath 和 css selector"></a>2.2.1 网页分析：<code>xpath</code> 和 <code>css</code> selector</h4><p>Scrapy默认提供了非常强大的<code>css</code>和<code>xpath</code> selector, 参考 <a target="_blank" rel="noopener" href="https://www.w3schools.com/xml/xpath_syntax.asp">W3Schools</a><br>已知页面的structure:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">  &lt;body&gt;</span><br><span class="line">    &lt;div&gt; &#x2F;&#x2F; Top Free App</span><br><span class="line">      &lt;h4&gt;Free&lt;&#x2F;h4&gt;</span><br><span class="line">      &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt;</span><br><span class="line">      ...</span><br><span class="line">    &lt;&#x2F;div&gt;</span><br><span class="line">    &lt;div&gt; &#x2F;&#x2F; Top Paid App</span><br><span class="line">      &lt;h4&gt;Free&lt;&#x2F;h4&gt;</span><br><span class="line">      &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt;</span><br><span class="line">      ...</span><br><span class="line">    &lt;&#x2F;div&gt;</span><br><span class="line">    &lt;div&gt; &#x2F;&#x2F; Top Grossing App</span><br><span class="line">      &lt;h4&gt;Free&lt;&#x2F;h4&gt;</span><br><span class="line">      &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt;</span><br><span class="line">      ...</span><br><span class="line">    &lt;&#x2F;div&gt;</span><br><span class="line">  &lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>
<p>其中每个<code>&lt;div&gt;</code>里面会列出类别以及包含50个app的rank<br>这里我们用到了四个:</p>
<ol>
<li><code>response.xpath(&#39;//div[count(a[contains(@href, &quot;en/apps&quot;)])&gt;=40]&#39;)</code>: 找到一个div并且里面有超过40个link，每个link都指向 <code>en/apps/...</code></li>
<li><code>css(&quot;h4::text&quot;)</code>: 提取 <code>h4</code> 里面的文字</li>
<li><code>xpath(&#39;.//a[contains(@href, &quot;en/apps&quot;)]&#39;)</code>: 提取链接</li>
<li><code>xpath(&#39;.//p/text()&#39;)</code>: 找到每个段落</li>
</ol>
<h4 id="2-3-运行"><a href="#2-3-运行" class="headerlink" title="2.3 运行"></a>2.3 运行</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; scrapy crawl top50</span><br></pre></td></tr></table></figure>
<p>可以看到我们能够拿到150个结果了</p>
<h4 id="2-4-输出-items"><a href="#2-4-输出-items" class="headerlink" title="2.4 输出 items"></a>2.4 输出 <code>items</code></h4><p>既然已经能够生成需要的数据，那么我们可以使用 <code>Pipeline</code> 来把处理过的内容输出到磁盘上</p>
<p>Filename: <code>app_rank/pipelines.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from datetime import datetime</span><br><span class="line">from itemadapter import ItemAdapter</span><br><span class="line">from scrapy.exporters import JsonLinesItemExporter</span><br><span class="line"></span><br><span class="line">class AppTrendPipeline:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.files &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        import os</span><br><span class="line">        print(os.getcwd())</span><br><span class="line">        spider.logger.info(&quot;Current path: %s&quot;, os.getcwd())</span><br><span class="line">        now &#x3D; datetime.now()</span><br><span class="line">        file &#x3D; open(&#39;json&#x2F;&#123;&#125;.json&#39;.format(now.strftime(&quot;%d_%m_%Y_%H_%M_%S&quot;)), &#39;w+b&#39;)</span><br><span class="line">        self.files[spider] &#x3D; file</span><br><span class="line">        self.exporter &#x3D; JsonLinesItemExporter(file)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        file &#x3D; self.files.pop(spider)</span><br><span class="line">        file.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<p>我们使用了 <code>JsonLinesItemExporter</code> 将每一个Item输出到了本地的 <code>.json</code> 文件中:</p>
<p><img src="/static/2021_03_17/2021_03_17_app_trend_list.png"></p>
<h4 id="2-5-通过网页控制台来运行我们的spider"><a href="#2-5-通过网页控制台来运行我们的spider" class="headerlink" title="2.5 通过网页控制台来运行我们的spider"></a>2.5 通过网页控制台来运行我们的spider</h4><p>访问 <a target="_blank" rel="noopener" href="http://localhost/">http://localhost</a> 应该就能看到控制台，以及我们增加了一个同为localhost的worker</p>
<p><img src="/static/2021_03_17/2021_03_17_scrapyweb.png"></p>
<ol>
<li>进入 屏幕左侧 <code>Deploy Project</code> 选项, 选择我们创建的 <code>app_trend</code> 项目，并且选择 <code>Package &amp; Deploy</code></li>
<li>进入 屏幕左侧 <code>Run Spider</code> 选项, 选择我们的项目和spider，点击 <code>Check CMD</code> 生成命令，这里也可以选择计划任务设置定时执行</li>
<li>点击 <code>Run Spider</code> 按钮</li>
</ol>
<p>第一个task开始运行，选择 <code>Jobs</code> 可以看到现在的jobs，点击第一个task的 <code>Stats</code>已经完成，我们抓取到了150个条目</p>
<p><img src="/static/2021_03_17/2021_03_17_first_task.png"></p>
</div></article></div><div class="right-container"><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2021/03/17/Trending-Apps-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E5%88%86%E6%9E%90%E6%B5%81%E8%A1%8C%E5%BA%94%E7%94%A8-%E9%87%87%E9%9B%86%E7%AF%87-%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE-1/" title="Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 1" class="next">下一篇</a></div><a href="#comment" class="comment-anchor"></a><div id="vcomments"></div><script>new Valine({
    el: "#vcomments",
    appId: "3B5inNt35pHjOqOh1kV2Dizv-MdYXbMMI",
    appKey: "IbOq02P13bE1ehn3a08IJmgQ",
    notify: false,
    verify: false,
    avatar: "robohash",
    visitor: true,
    placeholder: "随便说点什么～.～",
});</script><div class="copyright"><p>© 2016 - 2021 <a target="_blank">robot9</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> <br> and <a href="https://github.com/Bulandent/hexo-theme-bubuzou" target="_blank">hexo-theme-bubuzou</a></p><p> <span style="padding-right: 6px;">备案是什么 不存在的</span></p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/jquery-1.8.2.min.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/articleCatalog.js"></script><script src="https://bubuzou.oss-cn-shenzhen.aliyuncs.com/blog/202010/main.js"></script><script>const valineAPI = (() => {
try {
    AV.init("3B5inNt35pHjOqOh1kV2Dizv-MdYXbMMI", "IbOq02P13bE1ehn3a08IJmgQ");
} catch(error) {}
const isExist = (identity) => {
    identity = identity || getRealPath();
    let query = new AV.Query('Timer');
    return new Promise((resolve, reject) => {
    query.equalTo("identity", identity);
    query.find().then(results => {
        resolve(results.length > 0);
    }, error => reject(error));
    })
}

const _get = (identity) => {
    let query = null;
    if(identity && identity instanceof Array){
    let querys = [];
    for(let i = 0; i < identity.length; ++i) {
        querys[i] = new AV.Query('Timer');
        querys[i].equalTo('identity', identity[i]);
    }
    query = AV.Query.or.apply(null ,querys);
    } else {
    identity = identity || getRealPath();
    query = new AV.Query("Timer");
    query.equalTo("identity", identity);
    }

    return new Promise((resolve, reject) => {
    query.find()
    .then(results => resolve(results))
    .catch(error => reject(error))
    })
}

const create = (identity) => {
    identity = identity || getRealPath();
    return new Promise((resolve, reject) => {
    let Todo = AV.Object.extend('Timer');
    let todo = new Todo();
    todo.set("times", 1);
    todo.set("identity", identity);
    todo.save().then(res => resolve(true), error => reject(error));
    })
}

const update = (identity) => {
    identity = identity || getRealPath();
    return new Promise((resolve, reject) => {
    let query = new AV.Query('Timer');
    query.equalTo("identity", identity);
    query.find().then(todos => {
        todos.forEach(todo => {
        todo.set("times", todo.attributes.times + 1);
        });
        return AV.Object.saveAll(todos);
    }).then(todos => resolve(true), error => reject(error));
    })
}

return {
    isExist,
    _get,
    update,
    create
}
})()

const calcAndWriteTimes = () => {
let isPost = true;

let timerAllDOM = document.querySelectorAll(".article-timer");

if(isPost) {
    let identity = timerAllDOM[0].getAttribute("data-identity");
    valineAPI.isExist(identity)
    .then(exist => {
    if(exist) {
        return valineAPI.update(identity);
    }
    return new Promise(resolve => resolve(true));
    })
    .then( succuess => valineAPI._get(identity))
    .then( result => timerAllDOM[0].innerText = result[0].attributes.times)
    .catch(error => console.log(error.message))
    return ;
}

let timerDOMCache = {};

for(let timerDOM of timerAllDOM) {
    let identity = timerDOM.getAttribute("data-identity");
    if(timerDOMCache.hasOwnProperty(identity)){
    timerDOMCache[identity].dom.push(timerDOM);
    }else{
    timerDOMCache[identity] = {
        dom: [timerDOM],
        times: undefined
    };
    }
}

let identities = Object.keys(timerDOMCache);
valineAPI._get(identities).then(results => {
    for(let result of results) {
    let {identity, times} = result.attributes;
    timerDOMCache[identity].times = times;
    timerDOMCache[identity].dom.map(item => item.innerText = times);
    }
    for(let identity of identities) {
    if(timerDOMCache[identity].times){
        continue;
    }
    timerDOMCache[identity].dom.map(item => item.innerText = 1);
    valineAPI.create(identity);
    }
}).catch(error => console.log(error.message))
}

if(true){
calcAndWriteTimes();
}</script></body></html>