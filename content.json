{"meta":{"title":"Blog(robot9)","subtitle":"","description":"","author":"robot9","url":"https://robot9.github.io","root":"/"},"pages":[],"posts":[{"title":"Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 2","slug":"Trending-Apps-从头开始分析流行应用-采集篇-环境设置-2","date":"2021-03-18T00:22:00.000Z","updated":"2021-03-18T03:29:14.127Z","comments":true,"path":"2021/03/18/Trending-Apps-从头开始分析流行应用-采集篇-环境设置-2/","link":"","permalink":"https://robot9.github.io/2021/03/18/Trending-Apps-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E5%88%86%E6%9E%90%E6%B5%81%E8%A1%8C%E5%BA%94%E7%94%A8-%E9%87%87%E9%9B%86%E7%AF%87-%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE-2/","excerpt":"Link to previous chapter: 环境设置-1 2. 开始第一个 Scarpy 项目2.1 创建项目因为我们本地并没有scrapy的executable，需要启动一个ad-hoc的容器并在容器里面操作， 进入容器： 1&gt; docker-compose run --rm scrapy 现在开始创建我们的project 123&gt; cd &#x2F;app&gt; scrapy startproject app_trend&gt; cd app_trend 这样子会生成一个 app_trend folder, 其中主要的文件目录是: 1234567891011tutorial&#x2F; scrapy.cfg # deploy configuration file tutorial&#x2F; # project&#39;s Python module, you&#39;ll import your code from here __init__.py items.py # project items definition file middlewares.py # project middlewares file pipelines.py # project pipelines file settings.py # project settings file spiders&#x2F; # !!! a directory where you&#39;ll later put your spiders __init__.py 我们将会在 spiders/ folder 里面编写我们自己的 spider","text":"Link to previous chapter: 环境设置-1 2. 开始第一个 Scarpy 项目2.1 创建项目因为我们本地并没有scrapy的executable，需要启动一个ad-hoc的容器并在容器里面操作， 进入容器： 1&gt; docker-compose run --rm scrapy 现在开始创建我们的project 123&gt; cd &#x2F;app&gt; scrapy startproject app_trend&gt; cd app_trend 这样子会生成一个 app_trend folder, 其中主要的文件目录是: 1234567891011tutorial&#x2F; scrapy.cfg # deploy configuration file tutorial&#x2F; # project&#39;s Python module, you&#39;ll import your code from here __init__.py items.py # project items definition file middlewares.py # project middlewares file pipelines.py # project pipelines file settings.py # project settings file spiders&#x2F; # !!! a directory where you&#39;ll later put your spiders __init__.py 我们将会在 spiders/ folder 里面编写我们自己的 spider 2.2 第一个 spiderFilename: spiders/top50_spider.py 123456789101112131415161718192021222324252627282930313233343536import scrapyclass Top50Spider(scrapy.Spider): name &#x3D; &quot;top50&quot; def start_requests(self): urls &#x3D; [ &#39;https:&#x2F;&#x2F;www.appannie.com&#x2F;en&#x2F;apps&#x2F;ios&#x2F;top&#x2F;&#39; ] for url in urls: yield scrapy.Request( url&#x3D;url, callback&#x3D;self.parse, headers&#x3D;&#123; &quot;accept&quot;: &quot;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9&quot;, &quot;accept-language&quot;: &quot;zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8,en-GB;q&#x3D;0.7,en-US;q&#x3D;0.6,zh-TW;q&#x3D;0.5,ja;q&#x3D;0.4&quot;, &quot;accept-encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;cache-control&quot;: &quot;max-age&#x3D;0&quot;, &quot;sec-fetch-dest&quot;: &quot;document&quot;, &quot;sec-fetch-mode&quot;: &quot;navigate&quot;, &quot;sec-fetch-site&quot;: &quot;none&quot;, &quot;sec-fetch-user&quot;: &quot;?1&quot;, &quot;upgrade-insecure-requests&quot;: &quot;1&quot;, &quot;user-agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;89.0.4389.82 Safari&#x2F;537.36 Edg&#x2F;89.0.774.50&quot;, &#125;) def parse(self, response): for ranking in response.xpath(&#39;&#x2F;&#x2F;div[count(a[contains(@href, &quot;en&#x2F;apps&quot;)])&gt;&#x3D;40]&#39;): category &#x3D; ranking.css(&quot;h4::text&quot;).get() for app in ranking.xpath(&#39;.&#x2F;&#x2F;a[contains(@href, &quot;en&#x2F;apps&quot;)]&#39;): items &#x3D; app.xpath(&#39;.&#x2F;&#x2F;p&#x2F;text()&#39;).getall() yield &#123; &#39;category&#39;: category, &#39;rank&#39;: items[0], &#39;name&#39;: items[1], &#39;company&#39;: items[2], &#125; 这里面特别加上了一些headers，特别是`user-agent`是为了防止服务器认为我们是机器人而直接返回503 Error 2.2.1 网页分析：xpath 和 css selectorScrapy默认提供了非常强大的css和xpath selector, 参考 W3Schools已知页面的structure: 12345678910111213141516171819&lt;html&gt; &lt;body&gt; &lt;div&gt; &#x2F;&#x2F; Top Free App &lt;h4&gt;Free&lt;&#x2F;h4&gt; &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt; ... &lt;&#x2F;div&gt; &lt;div&gt; &#x2F;&#x2F; Top Paid App &lt;h4&gt;Free&lt;&#x2F;h4&gt; &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt; ... &lt;&#x2F;div&gt; &lt;div&gt; &#x2F;&#x2F; Top Grossing App &lt;h4&gt;Free&lt;&#x2F;h4&gt; &lt;a href&#x3D;&quot;en&#x2F;apps&#x2F;...&quot;&gt;&lt;p&gt;app_rank&lt;&#x2F;p&gt;&lt;p&gt;name&lt;&#x2F;p&gt;&lt;p&gt;company&lt;&#x2F;p&gt;&lt;&#x2F;a&gt; ... &lt;&#x2F;div&gt; &lt;&#x2F;body&gt;&lt;&#x2F;html&gt; 其中每个&lt;div&gt;里面会列出类别以及包含50个app的rank这里我们用到了四个: response.xpath(&#39;//div[count(a[contains(@href, &quot;en/apps&quot;)])&gt;=40]&#39;): 找到一个div并且里面有超过40个link，每个link都指向 en/apps/... css(&quot;h4::text&quot;): 提取 h4 里面的文字 xpath(&#39;.//a[contains(@href, &quot;en/apps&quot;)]&#39;): 提取链接 xpath(&#39;.//p/text()&#39;): 找到每个段落 2.3 运行1&gt; scrapy crawl top50 可以看到我们能够拿到150个结果了 2.4 输出 items既然已经能够生成需要的数据，那么我们可以使用 Pipeline 来把处理过的内容输出到磁盘上 Filename: app_rank/pipelines.py 1234567891011121314151617181920212223242526from datetime import datetimefrom itemadapter import ItemAdapterfrom scrapy.exporters import JsonLinesItemExporterclass AppTrendPipeline: def __init__(self): self.files &#x3D; &#123;&#125; def open_spider(self, spider): import os print(os.getcwd()) spider.logger.info(&quot;Current path: %s&quot;, os.getcwd()) now &#x3D; datetime.now() file &#x3D; open(&#39;json&#x2F;&#123;&#125;.json&#39;.format(now.strftime(&quot;%d_%m_%Y_%H_%M_%S&quot;)), &#39;w+b&#39;) self.files[spider] &#x3D; file self.exporter &#x3D; JsonLinesItemExporter(file) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() file &#x3D; self.files.pop(spider) file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item 我们使用了 JsonLinesItemExporter 将每一个Item输出到了本地的 .json 文件中: 2.5 通过网页控制台来运行我们的spider访问 http://localhost 应该就能看到控制台，以及我们增加了一个同为localhost的worker 进入 屏幕左侧 Deploy Project 选项, 选择我们创建的 app_trend 项目，并且选择 Package &amp; Deploy 进入 屏幕左侧 Run Spider 选项, 选择我们的项目和spider，点击 Check CMD 生成命令，这里也可以选择计划任务设置定时执行 点击 Run Spider 按钮 第一个task开始运行，选择 Jobs 可以看到现在的jobs，点击第一个task的 Stats已经完成，我们抓取到了150个条目","categories":[{"name":"app_trend","slug":"app-trend","permalink":"https://robot9.github.io/categories/app-trend/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://robot9.github.io/tags/docker/"},{"name":"scrapy","slug":"scrapy","permalink":"https://robot9.github.io/tags/scrapy/"},{"name":"scrapyweb","slug":"scrapyweb","permalink":"https://robot9.github.io/tags/scrapyweb/"}],"author":"robot9"},{"title":"Trending Apps: 从头开始分析流行应用 - 采集篇 - 环境设置 - 1","slug":"Trending-Apps-从头开始分析流行应用-采集篇-环境设置-1","date":"2021-03-17T21:20:00.000Z","updated":"2021-03-18T03:29:50.972Z","comments":true,"path":"2021/03/17/Trending-Apps-从头开始分析流行应用-采集篇-环境设置-1/","link":"","permalink":"https://robot9.github.io/2021/03/17/Trending-Apps-%E4%BB%8E%E5%A4%B4%E5%BC%80%E5%A7%8B%E5%88%86%E6%9E%90%E6%B5%81%E8%A1%8C%E5%BA%94%E7%94%A8-%E9%87%87%E9%9B%86%E7%AF%87-%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE-1/","excerpt":"项目目标收集最新的iOS App排名，并分析流行趋势 采集篇使用到的tools scrapy: 基于python的网页采集框架 scrapydweb: 用于 Scrapyd 集群管理的 web 应用，支持 Scrapy 日志分析和可视化。 docker: 多服务容器管理 1. 创建docker instance1.1 准备工作： 一台linux服务器 安装 docker 以及 docker-compose 工具 1.2 文件目录123456789101112131415161718app_trend &#x2F;code&#x2F; # 爬虫的python code放这儿 &#x2F;scrapy_web&#x2F; # scrapydweb 的 config，logs 以及 build file &#x2F;app&#x2F; # scrapydweb 的 config 文件 # 用来override https:&#x2F;&#x2F;github.com&#x2F;my8100&#x2F;scrapydweb&#x2F;blob&#x2F;master&#x2F;scrapydweb&#x2F;default_settings.py &#x2F;scrapydweb_settings_v10.py &#x2F;logs&#x2F; &#x2F;data&#x2F; &#x2F;Dockerfile &#x2F;scrapyd&#x2F; &#x2F;scrapyd.conf &#x2F;Dockerfile &#x2F;data&#x2F; # 远程调用scrapyd的任务的output会在这 &#x2F;code&#x2F; # 自定义启动scrapyd的脚本 &#x2F;entrypoint.sh","text":"项目目标收集最新的iOS App排名，并分析流行趋势 采集篇使用到的tools scrapy: 基于python的网页采集框架 scrapydweb: 用于 Scrapyd 集群管理的 web 应用，支持 Scrapy 日志分析和可视化。 docker: 多服务容器管理 1. 创建docker instance1.1 准备工作： 一台linux服务器 安装 docker 以及 docker-compose 工具 1.2 文件目录123456789101112131415161718app_trend &#x2F;code&#x2F; # 爬虫的python code放这儿 &#x2F;scrapy_web&#x2F; # scrapydweb 的 config，logs 以及 build file &#x2F;app&#x2F; # scrapydweb 的 config 文件 # 用来override https:&#x2F;&#x2F;github.com&#x2F;my8100&#x2F;scrapydweb&#x2F;blob&#x2F;master&#x2F;scrapydweb&#x2F;default_settings.py &#x2F;scrapydweb_settings_v10.py &#x2F;logs&#x2F; &#x2F;data&#x2F; &#x2F;Dockerfile &#x2F;scrapyd&#x2F; &#x2F;scrapyd.conf &#x2F;Dockerfile &#x2F;data&#x2F; # 远程调用scrapyd的任务的output会在这 &#x2F;code&#x2F; # 自定义启动scrapyd的脚本 &#x2F;entrypoint.sh 1.2 创建 scrapydweb 镜像Filename: scrapy_web/Dockerfile [展开文件] 123456789101112FROM python:3.8-slimWORKDIR &#x2F;appEXPOSE 5000 RUN apt-get update &amp;&amp; \\ apt-get install -y git &amp;&amp; \\ pip3 install -U git+https:&#x2F;&#x2F;github.com&#x2F;my8100&#x2F;scrapydweb.git &amp;&amp; \\ apt-get remove -y git# 通过这个来override一些dependency的version# RUN pip3 install SQLAlchemy&#x3D;&#x3D;1.3.23 --upgradeCMD scrapydweb 1.3 创建 scrapyd 镜像[启动logparser和scrapyd] Filename: scrapyd/code/entrypoint.sh 12#!&#x2F;bin&#x2F;bashlogparser -dir &#x2F;var&#x2F;lib&#x2F;scrapyd&#x2F;logs -t 10 --delete_json_files &amp; scrapyd [创建镜像] Filename: scrapyd/Dockerfile [展开文件] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667FROM debian:busterMAINTAINER EasyPi Software FoundationENV SCRAPY_VERSION&#x3D;2.4.1ENV SCRAPYD_VERSION&#x3D;1.2.1ENV PILLOW_VERSION&#x3D;8.1.0RUN set -xe \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y autoconf \\ build-essential \\ curl \\ git \\ libffi-dev \\ libssl-dev \\ libtool \\ libxml2 \\ libxml2-dev \\ libxslt1.1 \\ libxslt1-dev \\ python3 \\ python3-dev \\ python3-distutils \\ vim-tiny \\ &amp;&amp; apt-get install -y libtiff5 \\ libtiff5-dev \\ libfreetype6-dev \\ libjpeg62-turbo \\ libjpeg62-turbo-dev \\ liblcms2-2 \\ liblcms2-dev \\ libwebp6 \\ libwebp-dev \\ zlib1g \\ zlib1g-dev \\ &amp;&amp; curl -sSL https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py | python3 \\ &amp;&amp; pip install git+https:&#x2F;&#x2F;github.com&#x2F;scrapy&#x2F;scrapy.git@$SCRAPY_VERSION \\ git+https:&#x2F;&#x2F;github.com&#x2F;scrapy&#x2F;scrapyd.git@$SCRAPYD_VERSION \\ git+https:&#x2F;&#x2F;github.com&#x2F;scrapy&#x2F;scrapyd-client.git \\ git+https:&#x2F;&#x2F;github.com&#x2F;scrapinghub&#x2F;scrapy-splash.git \\ git+https:&#x2F;&#x2F;github.com&#x2F;scrapinghub&#x2F;scrapyrt.git \\ git+https:&#x2F;&#x2F;github.com&#x2F;python-pillow&#x2F;Pillow.git@$PILLOW_VERSION \\ &amp;&amp; pip install logparser \\ &amp;&amp; curl -sSL https:&#x2F;&#x2F;github.com&#x2F;scrapy&#x2F;scrapy&#x2F;raw&#x2F;master&#x2F;extras&#x2F;scrapy_bash_completion -o &#x2F;etc&#x2F;bash_completion.d&#x2F;scrapy_bash_completion \\ &amp;&amp; echo &#39;source &#x2F;etc&#x2F;bash_completion.d&#x2F;scrapy_bash_completion&#39; &gt;&gt; &#x2F;root&#x2F;.bashrc \\ &amp;&amp; apt-get purge -y --auto-remove autoconf \\ build-essential \\ curl \\ libffi-dev \\ libssl-dev \\ libtool \\ libxml2-dev \\ libxslt1-dev \\ python3-dev \\ &amp;&amp; apt-get purge -y --auto-remove libtiff5-dev \\ libfreetype6-dev \\ libjpeg62-turbo-dev \\ liblcms2-dev \\ libwebp-dev \\ zlib1g-dev \\ &amp;&amp; rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*EXPOSE 6800VOLUME [&quot;&#x2F;code&quot;]WORKDIR &#x2F;codeRUN [&quot;chmod&quot;, &quot;777&quot;, &quot;entrypoint.sh&quot;]ENTRYPOINT [&quot;.&#x2F;entrypoint.sh&quot;] [scrapyd的设置文件] Filename: scrapyd/scrapyd.conf [展开文件] 1234567891011121314151617181920212223242526272829[scrapyd]eggs_dir &#x3D; &#x2F;var&#x2F;lib&#x2F;scrapyd&#x2F;eggslogs_dir &#x3D; &#x2F;var&#x2F;lib&#x2F;scrapyd&#x2F;logsitems_dir &#x3D; &#x2F;var&#x2F;lib&#x2F;scrapyd&#x2F;itemsdbs_dir &#x3D; &#x2F;var&#x2F;lib&#x2F;scrapyd&#x2F;dbsjobs_to_keep &#x3D; 5max_proc &#x3D; 0max_proc_per_cpu &#x3D; 4finished_to_keep &#x3D; 100poll_interval &#x3D; 5#设置成0.0.0.0来允许外网访问bind_address &#x3D; 0.0.0.0http_port &#x3D; 6800debug &#x3D; offrunner &#x3D; scrapyd.runnerapplication &#x3D; scrapyd.app.applicationlauncher &#x3D; scrapyd.launcher.Launcher[services]schedule.json &#x3D; scrapyd.webservice.Schedulecancel.json &#x3D; scrapyd.webservice.Canceladdversion.json &#x3D; scrapyd.webservice.AddVersionlistprojects.json &#x3D; scrapyd.webservice.ListProjectslistversions.json &#x3D; scrapyd.webservice.ListVersionslistspiders.json &#x3D; scrapyd.webservice.ListSpidersdelproject.json &#x3D; scrapyd.webservice.DeleteProjectdelversion.json &#x3D; scrapyd.webservice.DeleteVersionlistjobs.json &#x3D; scrapyd.webservice.ListJobsdaemonstatus.json &#x3D; scrapyd.webservice.DaemonStatus 1.4 编写 docker-compose 文件来定义 containerFilename: docker-compose.yml 1234567891011121314151617181920212223242526272829303132333435363738394041scrapy: image: vimagick&#x2F;scrapyd:py3 command: bash volumes: - .&#x2F;code:&#x2F;code working_dir: &#x2F;code restart: unless-stoppedscrapy_web: container_name: scrapy_web restart: unless-stopped build: .&#x2F;scrapy_web&#x2F; ports: - &quot;80:80&quot; expose: - &quot;80&quot; volumes: - .&#x2F;scrapy_web&#x2F;app:&#x2F;app - .&#x2F;scrapy_web&#x2F;logs:&#x2F;logs - .&#x2F;scrapy_web&#x2F;data:&#x2F;data - .&#x2F;code:&#x2F;code environment: - PASSWORD - USERNAME # 填入本机IP 或者其他运行了爬虫的机器 - SCRAPYD_SERVER_1&#x3D;[Your_IP]:6800 - PORT&#x3D;80 - DATA_PATH&#x3D;&#x2F;data depends_on: - scrapydscrapyd: container_name: scrapyd build: .&#x2F;scrapyd ports: - &quot;6800:6800&quot; volumes: - .&#x2F;scrapyd&#x2F;scrapyd.conf:&#x2F;etc&#x2F;scrapyd&#x2F;scrapyd.conf - .&#x2F;scrapyd&#x2F;data:&#x2F;var&#x2F;lib&#x2F;scrapyd - .&#x2F;scrapyd&#x2F;code:&#x2F;code restart: unless-stopped 1.5 启动 docker 容器 1docker-compose up -d 现在可以通过80端口来访问啦 1.6 编写 scrapy 项目1docker-compose run --rm scrapy 会启动一个设置好scrapy的容器 在里面可以直接用 scrapy startproject tutorial 并测试你的spider啦而与此同时这个容器内的 /code 实际上对应的就是外部的 code folder，所以任何文件操作都会保存在这并能从外部访问","categories":[{"name":"app_trend","slug":"app-trend","permalink":"https://robot9.github.io/categories/app-trend/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://robot9.github.io/tags/docker/"},{"name":"scrapy","slug":"scrapy","permalink":"https://robot9.github.io/tags/scrapy/"},{"name":"scrapyweb","slug":"scrapyweb","permalink":"https://robot9.github.io/tags/scrapyweb/"}],"author":"robot9"}],"categories":[{"name":"app_trend","slug":"app-trend","permalink":"https://robot9.github.io/categories/app-trend/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://robot9.github.io/tags/docker/"},{"name":"scrapy","slug":"scrapy","permalink":"https://robot9.github.io/tags/scrapy/"},{"name":"scrapyweb","slug":"scrapyweb","permalink":"https://robot9.github.io/tags/scrapyweb/"}]}